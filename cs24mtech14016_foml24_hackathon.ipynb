{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88248,"databundleVersionId":10095947,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns4\nimport warnings\nfrom sklearn.preprocessing import StandardScaler\nwarnings.filterwarnings('ignore')","metadata":{"id":"bYZGt0aTDObc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/vnb-foml-2024-hackathon/foml24_hackathon/train.csv')\n#test = pd.read_csv('/kaggle/input/vnb-foml-2024-hackathon/foml24_hackathon/test.csv')\noutput = pd.read_csv('/kaggle/input/vnb-foml-2024-hackathon/foml24_hackathon/sample_submission.csv')","metadata":{"id":"KlU2JbPwPwlg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ysryhSyiPxzG","outputId":"664d019d-0dd3-4e80-ac04-5edc0fa9cfa4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dtypes.value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"kCFM8-PAPzRs","outputId":"7c426505-9227-43a7-da59-56a80325e8fe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding the number of nan entries in each column\nprint(train.isnull().sum())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v9v498TNQE1p","outputId":"db64af48-181a-4e5f-aade-cbc8763ef4bf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to see the class distribution\nsns.displot(train, x=\"Target\") #plot shows class imbalance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop(['WaterAccessPointsCalc', 'CropSpeciesVariety', 'PrimaryCropAreaSqft', 'CultivatedAreaSqft1', 'PerimeterGuardPlantsArea','CultivatedAndWildArea', 'FieldEstablishedYear', 'FarmClassification', 'FieldZoneLevel'], axis=1)\n","metadata":{"id":"TH2xqWGkQHWT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#delete nan columns\ntrain = train.dropna(axis=1, how='all')\n#test = test.dropna(axis=1, how='all')","metadata":{"id":"7c_s5qKtQJC_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove columns that have more missing values\nn_rows= train.shape[0]\n\nfor i in train.columns:\n  if train[i].isnull().sum() > 0.8*n_rows:\n    train.drop(i, axis=1, inplace=True)","metadata":{"id":"F_HJEeSQQKvc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#replacing the missing entries with median \nfor i in train.columns:\n  if train[i].dtype in ['int64', 'float64']:\n    if train[i].isnull().sum() > 0:\n      train[i].fillna(train[i].median(), inplace=True)","metadata":{"id":"tH85G6VxQMZ3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding correlation of each feature with the target class label","metadata":{}},{"cell_type":"code","source":"df = train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['corr_label'] = (train['Target'] == 'high').astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['corr_label']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_to_exclude = 'corr_label'  \nnumerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.difference([column_to_exclude])\n\ncorrelations = {}\n\nfor col in numerical_columns:\n    correlation = df[col].corr(df['corr_label'])\n    correlations[col] = correlation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ncorrelations_series = pd.Series(correlations)\n\n# Plotting the correlations as a bar plot\nplt.figure(figsize=(10, 6))\ncorrelations_series.sort_values().plot(kind='barh', color='steelblue')\nplt.title('Correlation between Numerical Features and High class label')\nplt.xlabel('Correlation Value')\nplt.ylabel('Features')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['corr_label'] = (train['Target'] == 'low').astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['corr_label']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_to_exclude = 'corr_label'  \nnumerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.difference([column_to_exclude])\n\ncorrelations = {}\n\nfor col in numerical_columns:\n    correlation = df[col].corr(df['corr_label'])\n    correlations[col] = correlation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ncorrelations_series = pd.Series(correlations)\n\n# Plotting the correlations as a bar plot\nplt.figure(figsize=(10, 6))\ncorrelations_series.sort_values().plot(kind='barh', color='steelblue')\nplt.title('Correlation between Numerical Features and Low class label')\nplt.xlabel('Correlation Value')\nplt.ylabel('Features')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['corr_label'] = (train['Target'] == 'medium').astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_to_exclude = 'corr_label'  \nnumerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.difference([column_to_exclude])\n\ncorrelations = {}\n\nfor col in numerical_columns:\n    correlation = df[col].corr(df['corr_label'])\n    correlations[col] = correlation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ncorrelations_series = pd.Series(correlations)\n\n# Plotting the correlations as a bar plot\nplt.figure(figsize=(10, 6))\ncorrelations_series.sort_values().plot(kind='barh', color='steelblue')\nplt.title('Correlation between Numerical Features and Medium class label')\nplt.xlabel('Correlation Value')\nplt.ylabel('Features')\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop('corr_label', axis=1, errors='ignore') \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the dataset into independent and target variables","metadata":{"id":"2NKg9T4nYkin"}},{"cell_type":"code","source":"X = train.drop(['Target'], axis = 1)\ny = train['Target']","metadata":{"id":"L8Tvn-njQQJb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"AzXE5HhtVLNV","outputId":"ea0c815e-a98f-4400-9ec9-80147984c089","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"id":"vLiLY8ueeMvM","outputId":"566220b1-a594-42dd-80d3-51e160d86caa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = StandardScaler()\nX_train = sc.fit_transform(X)","metadata":{"id":"fyKQJdxnv78V","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random forest model","metadata":{}},{"cell_type":"code","source":"#random forest before label encoding y\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier()\nclassifier.fit(X_train, y)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":80},"id":"f31jtQShvyr7","outputId":"25d2cb72-e2bb-4aab-c9f6-4938559c1754","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Label encoding the target variable","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)","metadata":{"id":"CHS85jzkrK1D","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.bincount(y_encoded))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iXay2QTYreXm","outputId":"ab853fbd-5627-4a32-988e-b62ed34c5f06","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_encoded","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6PpHKKbnrkJU","outputId":"4d3b4d9b-c14d-4391-b4ce-894f49e9066a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGBoost model ","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\n\nmodel_xgboost = XGBClassifier(\n    objective='multi:softmax', \n    num_class=3,              #since the target has 3 classes \n    learning_rate=0.1,\n    max_depth=5,\n    n_estimators=5000,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    eval_metric='mlogloss'     \n)","metadata":{"id":"q9wsfEgcrrAy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_xgboost.fit(X_train, y_encoded)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"id":"_Hc1wiqBrwe-","outputId":"c581c128-5c68-4c3c-c72c-168e9ee678e7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n\ndef make_predictions(test_fname, predictions_fname):\n    \n    test = pd.read_csv(test_fname)\n    \n    #preprocessing test set\n    test = test.drop(['WaterAccessPointsCalc', 'CropSpeciesVariety', 'PrimaryCropAreaSqft', 'CultivatedAreaSqft1', 'PerimeterGuardPlantsArea','CultivatedAndWildArea', 'FieldEstablishedYear', 'FarmClassification', 'FieldZoneLevel'], axis=1)\n    test = test.dropna(axis=1, how='all')\n    n_rows= test.shape[0]\n\n    for i in test.columns:\n      if test[i].isnull().sum() > 0.8*n_rows:\n        test.drop(i, axis=1, inplace=True)\n        \n    #the mean value is significantly lower than the max value. hence, we replace with median as this dataset has extreme outliers\n    for i in test.columns:\n      if test[i].dtype in ['int64', 'float64']:\n        if test[i].isnull().sum() > 0:\n          test[i].fillna(test[i].median(), inplace=True)\n    \n    X_test = sc.transform(test)\n    \n    #Random Forest model\n    rf_pred = classifier.predict(X_test)  \n\n    #XGBoost model\n    predictions = model_xgboost.predict(X_test)\n    pred_labels = label_encoder.inverse_transform(predictions)\n    \n    #ensembling both Random Forest and XGBoosting\n    predictions_df = pd.DataFrame({'xgb_pred': pred_labels, 'rf_pred': rf_pred})\n    voted_pred = predictions_df.mode(axis=1)[0] \n\n    voted_pred = voted_pred.values\n\n    final = pd.DataFrame()\n    final['UID'] = test['UID']\n    final['Target'] = voted_pred  # Use the voted predictions here\n\n    final.to_csv('predictions_fname.csv', index=False)\n\n    #### end code ####\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--train-file\", type=str, help='file path of train.csv')\n    parser.add_argument(\"--test-file\", type=str, help='file path of test.csv')\n    parser.add_argument(\"--predictions-file\", type=str, help='save path of predictions')\n    args = parser.parse_args()\n\n    make_predictions(args.test_file, args.predictions_file)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}